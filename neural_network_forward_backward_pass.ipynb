{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# --- 1. Eingaben und Initialisierung der Parameter ---\n",
    "# Eingabedaten\n",
    "X = np.array([\n",
    "    [1, 2, -1],\n",
    "    [0, -1, 3],\n",
    "    [2, 0, 1]\n",
    "])\n",
    "\n",
    "# Gewichte und Biases der ersten Schicht (Initialzustand)\n",
    "W1 = np.array([\n",
    "    [0.2, -0.5],\n",
    "    [1.0, 0.3],\n",
    "    [-1.5, 2.0]\n",
    "])\n",
    "b1 = np.array([\n",
    "    [0.5, -1.0]\n",
    "])\n",
    "\n",
    "# Gewichte und Biases der zweiten Schicht (Initialzustand)\n",
    "W2 = np.array([\n",
    "    [1.0],\n",
    "    [-1.2]\n",
    "])\n",
    "b2 = np.array([\n",
    "    [0.3]\n",
    "])\n",
    "\n",
    "# Wahre Labels (für die Verlustberechnung)\n",
    "Y = np.array([\n",
    "    [1],\n",
    "    [0],\n",
    "    [1]\n",
    "])\n",
    "\n",
    "# Lernrate\n",
    "eta = 0.1\n",
    "\n",
    "print(\"--- Initialzustand der Parameter und Eingaben ---\")\n",
    "print(\"X:\\n\", X)\n",
    "print(\"W1:\\n\", W1)\n",
    "print(\"b1:\\n\", b1)\n",
    "print(\"W2:\\n\", W2)\n",
    "print(\"b2:\\n\", b2)\n",
    "print(\"Y:\\n\", Y)\n",
    "print(\"Lernrate (eta):\", eta)\n",
    "print(\"-\" * 40)\n",
    "\n",
    "\n",
    "# --- 2. Vorwärtspass-Berechnungen ---\n",
    "print(\"\\n--- Vorwärtspass-Berechnungen ---\")\n",
    "\n",
    "# Berechnung von Z1 = X * W1 + b1\n",
    "Z1 = np.dot(X, W1) + b1\n",
    "print(\"Z1 (Pre-Aktivierung 1):\\n\", Z1)\n",
    "\n",
    "# Aktivierung A1 = ReLU(Z1)\n",
    "A1 = np.maximum(0, Z1)\n",
    "print(\"A1 (Aktivierung 1):\\n\", A1)\n",
    "\n",
    "# Berechnung von Z2 = A1 * W2 + b2\n",
    "Z2 = np.dot(A1, W2) + b2\n",
    "print(\"Z2 (Pre-Aktivierung 2):\\n\", Z2)\n",
    "\n",
    "# Ausgabe A2 = sigmoid(Z2) (Sigmoid-Aktivierung der Output-Schicht)\n",
    "A2 = 1 / (1 + np.exp(-Z2))\n",
    "print(\"A2 (Output-Aktivierung):\\n\", A2)\n",
    "print(\"-\" * 40)\n",
    "\n",
    "\n",
    "# --- 3. Backpropagation-Berechnungen (Gradienten) ---\n",
    "print(\"\\n--- Backpropagation-Berechnungen (Gradienten) ---\")\n",
    "\n",
    "##### Schritt 1: Fehler am Ausgangsneuron berechnen ($\\delta_2$)\n",
    "# Für Sigmoid-Output und (angenommenem) Kreuzentropie-Verlust\n",
    "delta2 = A2 - Y\n",
    "print(\"\\ndelta2 (Fehler der Output-Schicht):\\n\", delta2)\n",
    "\n",
    "##### Schritt 2: Gradienten für W2 und b2\n",
    "# Gradient der Gewichte W2\n",
    "grad_W2 = np.dot(A1.T, delta2)\n",
    "print(\"\\ngrad_W2:\\n\", grad_W2)\n",
    "\n",
    "# Gradient des Bias b2 (Mittelwert über die Batch-Dimension)\n",
    "grad_b2 = np.mean(delta2, axis=0)\n",
    "print(\"\\ngrad_b2:\\n\", grad_b2)\n",
    "\n",
    "\n",
    "##### Schritt 3: Gradienten für W1 und b1\n",
    "# Berechnung von delta1 (Fehler der Hidden-Schicht)\n",
    "# delta1 = (delta2 .dot W2.T) o f'(Z1)\n",
    "relu_prime_Z1 = (Z1 > 0).astype(float) # Ableitung der ReLU-Funktion (1 für x>0, 0 sonst)\n",
    "\n",
    "delta1_intermediate = np.dot(delta2, W2.T)\n",
    "delta1 = delta1_intermediate * relu_prime_Z1 # Elementweises Produkt (Hadamard)\n",
    "print(\"\\ndelta1 (Fehler der Hidden-Schicht):\\n\", delta1)\n",
    "\n",
    "# Gradient der Gewichte W1\n",
    "grad_W1 = np.dot(X.T, delta1)\n",
    "print(\"\\ngrad_W1:\\n\", grad_W1)\n",
    "\n",
    "# Gradient des Bias b1 (Mittelwert über die Batch-Dimension)\n",
    "grad_b1 = np.mean(delta1, axis=0)\n",
    "print(\"\\ngrad_b1:\\n\", grad_b1)\n",
    "print(\"-\" * 40)\n",
    "\n",
    "\n",
    "# --- 4. Abschluss der Backpropagation: Parameter-Updates ---\n",
    "print(\"\\n--- Parameter-Updates ---\")\n",
    "\n",
    "# Update W2\n",
    "W2_updated = W2 - eta * grad_W2\n",
    "print(\"\\nW2_aktualisiert:\\n\", W2_updated)\n",
    "\n",
    "# Update b2\n",
    "b2_updated = b2 - eta * grad_b2\n",
    "print(\"\\nb2_aktualisiert:\\n\", b2_updated)\n",
    "\n",
    "# Update W1\n",
    "W1_updated = W1 - eta * grad_W1\n",
    "print(\"\\nW1_aktualisiert:\\n\", W1_updated)\n",
    "\n",
    "# Update b1\n",
    "b1_updated = b1 - eta * grad_b1\n",
    "print(\"\\nb1_aktualisiert:\\n\", b1_updated)\n",
    "print(\"-\" * 40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
