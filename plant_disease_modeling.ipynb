{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center; font-family: Cambria, 'Times New Roman', serif; color: black;\">\n",
    "\n",
    "\n",
    "  <h1 style=\"font-family: Arial, sans-serif; font-size: 30pt; margin: 40px 0 10px 0;\">\n",
    "    Automatisierte Erkennung von Pflanzenkrankheiten\n",
    "  </h1>\n",
    "  \n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Die frühzeitige Erkennung von Pflanzenkrankheiten ist entscheidend für die Sicherung landwirtschaftlicher Erträge. Maschinelles Lernen (ML) bietet moderne Methoden, um den Gesundheitszustand von Pflanzen automatisiert zu klassifizieren und dadurch wirtschaftliche Schäden zu minimieren.\n",
    "\n",
    "In dieser Arbeit werden klassische ML-Modelle sowie künstliche neuronale Netze (ANN, CNN) auf Bilddaten aus dem PlantVillage-Datensatz angewendet und verglichen. Ziel ist es, die Effektivität verschiedener Modellierungsansätze zur Erkennung gesunder und kranker Blätter – sowohl einzeln pro Pflanzentyp als auch in einem kombinierten Klassifikator – zu evaluieren. Neben der praktischen Umsetzung wird ein besonderer Fokus auf den mathematischen Hintergrund neuronaler Netzwerke gelegt.\n",
    "\n",
    "\n",
    "Die Landwirtschaft steht vor der Herausforderung, Pflanzenkrankheiten frühzeitig zu erkennen, um Ernteausfälle zu verhindern und den Einsatz von Pflanzenschutzmitteln effizient zu gestalten. Besonders Kleinbauern sind von solchen Verlusten wirtschaftlich stark betroffen. Eine automatisierte Erkennung von Krankheitsbildern mithilfe von Bildanalyse kann in diesem Zusammenhang eine entscheidende Rolle spielen.\n",
    "\n",
    "Maschinelles Lernen (ML) hat sich als leistungsfähige Methode zur Klassifikation und Mustererkennung etabliert – auch in der Pflanzenpathologie. Insbesondere durch den Einsatz von Deep-Learning-Modellen können Bilder von Pflanzenblättern analysiert und der Gesundheitszustand der Pflanze präzise bestimmt werden. Dies ermöglicht eine frühzeitige Diagnose und gezielte Gegenmaßnahmen zur Ertragssicherung."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verwendete Bibliotheken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System- und Dateiverwaltung\n",
    "import os                 # Interaktion mit dem Betriebssystem (Pfade, Ordner)\n",
    "import shutil             # Datei- und Verzeichnisoperationen (Kopieren, Verschieben)\n",
    "from PIL import Image     # Bildbearbeitung\n",
    "import cv2                # Computer Vision (OpenCV)\n",
    "import pywt               # Wavelet-Transformationen\n",
    "\n",
    "# Basis-Bibliotheken & Visualisierung\n",
    "import numpy as np        # Numerische Berechnungen (Arrays, Matrizen)\n",
    "import random\n",
    "import pandas as pd       # Datenanalyse und -manipulation (DataFrames)\n",
    "import matplotlib.pyplot as plt  # Datenvisualisierung (Diagramme)\n",
    "import matplotlib.image as mpimg # Bilder laden/anzeigen\n",
    "\n",
    "# Preprocessing, Split & Evaluation (Scikit-learn)\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold # Datensatz-Splitting, Hyperparameter-Optimierung, Kreuzvalidierung\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder             # Daten skalieren, Labels kodieren\n",
    "from sklearn.metrics import (                                             # Metriken zur Modellbewertung \n",
    "    confusion_matrix,      \n",
    "    ConfusionMatrixDisplay,\n",
    "    accuracy_score,\n",
    "    classification_report\n",
    ")\n",
    "\n",
    "# Klassische ML-Modelle & Pipeline (Scikit-learn)\n",
    "from sklearn.linear_model import LogisticRegression  \n",
    "from sklearn.svm import SVC                          \n",
    "from sklearn.tree import DecisionTreeClassifier      \n",
    "from sklearn.ensemble import RandomForestClassifier   \n",
    "from sklearn.pipeline import make_pipeline             # Erstellung von Verarbeitungspipelines\n",
    "import joblib                                          # Speichern/Laden von Python-Objekten (Modellen)\n",
    "\n",
    "# Deep Learning (Keras mit TensorFlow)\n",
    "import tensorflow as tf                                # Haupt-Deep-Learning-Framework\n",
    "from keras import models, layers                       # Modelldefinition (Schichten, Architektur)\n",
    "from keras.optimizers import AdamW                     # Optimierungsalgorithmus\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau # Callback für Training (z.B. Frühzeitiges Stoppen, Lernraten-Anpassung)\n",
    "from keras.models import load_model                    # Laden von gespeicherten Keras-Modellen\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Künstliches Neuronales Netz – Eigenständige Implementierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_params():\n",
    "    \"\"\"\n",
    "    Initialisiert Gewichte und Biases mit Zufallswerten und Nullen.\n",
    "    \"\"\"\n",
    "\n",
    "    # W1: Gewichtsmatrix von der Eingabe- (2 Features) zur ersten versteckten Schicht (2 Neuronen)\n",
    "    w1 = np.random.randn(2, 2) * np.sqrt(2 / 2) \n",
    "\n",
    "    # b1: Bias-Vektor für die erste versteckte Schicht (2 Neuronen)\n",
    "    b1 = np.zeros((1, 2))  \n",
    "\n",
    "    # W2: Gewichtsmatrix von der ersten versteckten Schicht (2 Neuronen) zur Ausgabe-Schicht (1 Neuron)\n",
    "    w2 = np.random.randn(2, 1) * np.sqrt(2 / 2)\n",
    "\n",
    "    # b2: Bias-Vektor für die Ausgabe-Schicht (1 Neuron)\n",
    "    b2 = np.zeros((1, 1))                     \n",
    "    return w1, w2, b1, b2\n",
    "\n",
    "def relu(Z):\n",
    "    \"\"\"ReLU Aktivierungsfunktion\"\"\"\n",
    "    return np.maximum(0, Z)\n",
    "\n",
    "def relu_derivative(Z):\n",
    "    \"\"\"Ableitung der ReLU Funktion\"\"\"\n",
    "    return (Z > 0).astype(float)\n",
    "\n",
    "def sigmoid(Z):\n",
    "    \"\"\"Sigmoid Aktivierungsfunktion\"\"\"\n",
    "    return 1 / (1 + np.exp(-Z))\n",
    "\n",
    "def forward(X, w1, w2, b1, b2):\n",
    "    \"\"\"\n",
    "    Forward-Pass: Berechnet Zwischenergebnisse und Vorhersagen.\n",
    "    \"\"\"\n",
    "\n",
    "    # Erste Schicht\n",
    "    z1 = np.dot(X, w1) + b1\n",
    "    a1 = relu(z1)   \n",
    "\n",
    "    # Zweite (Ausgabe-) Schicht\n",
    "    z2 = np.dot(a1, w2) + b2\n",
    "    a2 = sigmoid(z2)\n",
    "\n",
    "    return z1, a1, z2, a2\n",
    "\n",
    "def train(X, Y, epochs=10000, learning_rate=0.1):  # Das Netzwerk wird über 'epochs' Iterationen trainiert\n",
    "    \"\"\"\n",
    "    Trainiert das Netzwerk mit Gradient Descent und Backpropagation.\n",
    "    \"\"\"\n",
    "    w1, w2, b1, b2 = initialize_params()\n",
    "    \n",
    "    for _ in range(epochs):\n",
    "\n",
    "        # Forward-Pass\n",
    "        z1, a1, z2, a2 = forward(X, w1, w2, b1, b2)\n",
    "        \n",
    "         # Backpropagation\n",
    "        # Fehler am Ausgang (Output-Schicht). Die Sigmoid-Funktion steht am Ausgang.\n",
    "        delta2 = a2 - Y # Dies ist der Gradient des Binary Cross-Entropy-Verlustes bzgl. Z2 für Sigmoid-Output\n",
    "        \n",
    "        \n",
    "        # Gradienten für w2, b2\n",
    "        dW2 = np.dot(a1.T, delta2)   # Matrixmultiplikation: Transponierte A1 und delta2\n",
    "        db2 = np.mean(delta2, axis=0, keepdims=True) # Mittelwert von delta2 über die Samples\n",
    "        \n",
    "      # Fehler zurück zu Schicht 1 propagieren\n",
    "        delta1 = np.dot(delta2, w2.T) * relu_derivative(z1) # Elementweises Produkt mit Ableitung der ReLU-Aktivierung\n",
    "        \n",
    "        # Gradienten für w1, b1\n",
    "        dW1 = np.dot(X.T, delta1)\n",
    "        db1 = np.mean(delta1, axis=0, keepdims=True)\n",
    "        \n",
    "        # Parameter-Update (Gradientenabstieg)\n",
    "        w2 -= learning_rate * dW2\n",
    "        b2 -= learning_rate * db2\n",
    "        w1 -= learning_rate * dW1\n",
    "        b1 -= learning_rate * db1\n",
    "    \n",
    "    # Nach 'epochs' Trainingsdurchläufen werden die aktualisierten Gewichte/Biases zurückgegeben,\n",
    "    # bereit für die Vorhersage der Daten.\n",
    "    \n",
    "    return w1, b1, w2, b2\n",
    "\n",
    "def predict(X, w1, w2, b1, b2):\n",
    "    \"\"\"\n",
    "    Nutzt das trainierte Netzwerk, um Vorhersagen zu treffen.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Führt den Forward-Pass aus, um die finalen Aktivierungen (a2) zu erhalten\n",
    "    _, _, _, a2 = forward(X, w1, w2, b1, b2) \n",
    "    # Konvertiert die Wahrscheinlichkeiten in binäre Vorhersagen (0 oder 1)\n",
    "    return (a2 > 0.5).astype(int)\n",
    "\n",
    "# Beispiel-Daten XOR\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "Y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "# --- Training des Netzwerks ---\n",
    "print(\"Starte Training des XOR-Netzwerks...\")\n",
    "w1, b1, w2, b2 = train(X, Y)\n",
    "print(\"Training abgeschlossen.\")\n",
    "\n",
    "# --- Vorhersage mit dem trainierten Netzwerk ---\n",
    "predictions = predict(X, w1, w2, b1, b2)\n",
    "print(\"\\n--- Vorhersagen für XOR-Eingaben ---\")\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##  Datensatz & Preprocessing\n",
    "### Datenauswahl und -aufbereitung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pfad zum ursprünglichen PlantVillage-Datensatz\n",
    "global_source_dir = 'data/PlantVillage'\n",
    "\n",
    "# Zielverzeichnis für die gefilterten und strukturierten Bilddaten\n",
    "global_target_dir = 'data_selected'\n",
    "\n",
    "# Auswahl: 3 Pflanzenarten (Tomate, Apfel, Mais) mit jeweils 3 Klassen (gesund + 2 Krankheiten)\n",
    "selected_categories = [\n",
    "    'Tomato___healthy',\n",
    "    'Tomato___Bacterial_spot',\n",
    "    'Tomato___Late_blight',\n",
    "    'Apple___healthy',\n",
    "    'Apple___Apple_scab',\n",
    "    'Apple___Black_rot',\n",
    "    'Corn___healthy',\n",
    "    'Corn___Common_rust',\n",
    "    'Corn___Northern_Leaf_Blight'\n",
    "]\n",
    "\n",
    "# Bildgröße zur Weiterverarbeitung – ursprünglich 256x256, hier auf 64x64 reduziert,\n",
    "# um die Trainings- und Rechenzeit deutlich zu verringern\n",
    "global_image_size = (64, 64)\n",
    "\n",
    "# Detailtiefe der Wavelet-Transformation (für spätere Feature-Extraktion)\n",
    "global_w2d_level = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_categories_grouped(source_dir, target_dir, categories):\n",
    "    \n",
    "    # Prüfen, ob der Quellordner existiert\n",
    "    if not os.path.exists(source_dir):\n",
    "        print(\"Datenordner existiert nicht!\")\n",
    "        return \n",
    "    \n",
    "    # Zielordner neu anlegen (falls vorhanden, wird er gelöscht)\n",
    "    if os.path.exists(target_dir):\n",
    "        shutil.rmtree(target_dir)\n",
    "    os.makedirs(target_dir)\n",
    "    \n",
    "    total_images = 0\n",
    "\n",
    "    # Schleife über alle ausgewählten Kategorien\n",
    "    for category in categories:\n",
    "\n",
    "        # Pfad zur Quellkategorie, z. B. data/PlantVillage/Tomato___healthy\n",
    "        src_path = os.path.join(source_dir, category)\n",
    "        \n",
    "        # Pflanzenname extrahieren, z. B. 'Tomato' aus 'Tomato___healthy'\n",
    "        plant_name = category.split('___')[0].lower()\n",
    "        \n",
    "        # Zielpfad, z. B. data_selected/tomato/raw_images/Tomato___healthy\n",
    "        dst_dir = os.path.join(target_dir, plant_name, 'raw_images', category)\n",
    "        \n",
    "        if os.path.exists(src_path):\n",
    "            os.makedirs(dst_dir, exist_ok=True)\n",
    "            \n",
    "            image_count = 0  # Zähler für Bilder in dieser Kategorie\n",
    "            \n",
    "            # Alle Bilddateien einzeln kopieren, um die Zielstruktur sauber aufzubauen\n",
    "            for filename in os.listdir(src_path):\n",
    "                # z. B. data/PlantVillage/Tomato___healthy/image (1).JPG\n",
    "                src_file = os.path.join(src_path, filename)\n",
    "                # z. B. data_selected/tomato/raw_images/Tomato___healthy/image (1).JPG\n",
    "                dst_file = os.path.join(dst_dir, filename)\n",
    "                if os.path.isfile(src_file):\n",
    "                    shutil.copy2(src_file, dst_file)\n",
    "                    image_count += 1\n",
    "            \n",
    "            total_images += image_count\n",
    "            print(f\"Kategorie '{category}': {image_count} Bilder nach '{dst_dir}' kopiert.\")\n",
    "        else:\n",
    "            print(f\"Achtung: Kategorie '{category}' nicht gefunden!\")\n",
    "    \n",
    "    print(f\"\\nInsgesamt kopierte Bilder: {total_images}\")\n",
    "\n",
    "\n",
    "# Funktionsaufruf mit den festgelegten Parametern\n",
    "select_categories_grouped(\n",
    "    source_dir=global_source_dir,\n",
    "    target_dir=global_target_dir,\n",
    "    categories=selected_categories\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zur explorativen Datenanalyse wird aus jeder ausgewählten Klasse ein zufälliges Bild visualisiert. Dies dient der Veranschaulichung der visuellen Unterschiede zwischen den Krankheitskategorien."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sortieren nach Klassenname\n",
    "all_classes = [\n",
    "    'data_selected/apple/raw_images/Apple___Apple_scab',\n",
    "    'data_selected/apple/raw_images/Apple___Black_rot',\n",
    "    'data_selected/apple/raw_images/Apple___healthy',\n",
    "    'data_selected/corn/raw_images/Corn___Northern_Leaf_Blight',\n",
    "    'data_selected/corn/raw_images/Corn___Common_rust',\n",
    "    'data_selected/corn/raw_images/Corn___healthy',\n",
    "    'data_selected/tomato/raw_images/Tomato___Late_blight',\n",
    "    'data_selected/tomato/raw_images/Tomato___Bacterial_spot',\n",
    "    'data_selected/tomato/raw_images/Tomato___healthy',\n",
    "]\n",
    "all_classes = sorted(all_classes)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "for i, class_path in enumerate(all_classes):\n",
    "    class_name = os.path.basename(class_path)\n",
    "\n",
    "    # Zufallszahl zwischen 1 und 100\n",
    "    rand_index = random.randint(1, 100)\n",
    "    \n",
    "    # Dateiname zusammensetzen\n",
    "    img_name = f\"image ({rand_index}).JPG\"\n",
    "    img_path = os.path.join(class_path, img_name)\n",
    "    print(img_path)\n",
    "\n",
    "    try:\n",
    "        img = mpimg.imread(img_path)\n",
    "        plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(img)\n",
    "        plt.title(class_name.replace('___', '\\n'), fontsize=10)\n",
    "        plt.axis('off')\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Bild {img_name} in {class_name} nicht gefunden.\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Feature-Reduktion durch Wavelet-Transformation***\n",
    "\n",
    "Die Wavelet-Transformation (z. B. mit Haar-Wavelets) extrahiert strukturierte Informationen wie Kanten oder Texturen aus Bildern, was besonders für die Klassifikation von Blattkrankheiten nützlich ist.  \n",
    "Sie reduziert dabei die Datenmenge und hebt relevante Muster hervor – oft mit besserer Modellleistung bei geringerem Rechenaufwand als bei Rohpixeln.\n",
    "\n",
    "Die Funktion w2d führt eine Haar-Wavelet-Transformation auf einem Bild durch und extrahiert gezielt Kanten- und Texturinformationen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2d(img, mode='haar', level=1):\n",
    "    \"\"\"\n",
    "    Führt eine Wavelet-Transformation auf einem RGB-Bild durch.\n",
    "    Gibt ein uint8-Graustufenbild mit extrahierten Texturinformationen zurück.\n",
    "    \"\"\"\n",
    "    # In Graustufen umwandeln, falls RGB\n",
    "    if len(img.shape) == 3:\n",
    "        img_gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    else:\n",
    "        img_gray = img.copy()\n",
    "\n",
    "    # In float konvertieren und normalisieren\n",
    "    img_gray = np.float32(img_gray) / 255.0\n",
    "\n",
    "    # Wavelet-Dekomposition\n",
    "    coeffs = pywt.wavedec2(img_gray, mode, level=level)\n",
    "\n",
    "    # Nur die Detailkoeffizienten verwenden\n",
    "    coeffs_H = list(coeffs)\n",
    "    coeffs_H[0] *= 0  # Approximationskoeffizienten auf 0 setzen\n",
    "\n",
    "    # Rekonstruktion der Details\n",
    "    img_reconstructed = pywt.waverec2(coeffs_H, mode)\n",
    "    img_reconstructed = np.clip(img_reconstructed * 255, 0, 255).astype(np.uint8)\n",
    "\n",
    "    return img_reconstructed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Zur Vorbereitung wurden alle Bilder einheitlich skaliert und in zwei Varianten verarbeitet:\n",
    "\n",
    "- **Graustufen**: Reduktion der Farbkanäle für einfachere Merkmalsextraktion.\n",
    "- **Wavelet-Transformation**: Extraktion von Textur- und Kanteninformationen.\n",
    "\n",
    "Die verarbeiteten Bilder wurden in passenden Ordnern gespeichert und später zu flachen Feature-Vektoren umgewandelt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_images(source_root=global_target_dir, image_size=global_image_size, \n",
    "                   processing_type='grayscale', mode='haar', level=global_w2d_level):\n",
    "    total_images = 0\n",
    "\n",
    "    # Prüfen, ob Quelldaten vorhanden sind\n",
    "    if not os.path.exists(source_root):\n",
    "        print(\"Datenordner existiert nicht!\")\n",
    "        return \n",
    "    \n",
    "    # Zielordnername je nach Verarbeitungsart setzen\n",
    "    output_folder = 'grayscale_features' if processing_type == 'grayscale' else 'wavelet_features'\n",
    "    \n",
    "    for plant_dir in os.listdir(source_root):\n",
    "        plant_path = os.path.join(source_root, plant_dir)\n",
    "        raw_images_path = os.path.join(plant_path, 'raw_images')\n",
    "        output_path = os.path.join(plant_path, output_folder)\n",
    "        \n",
    "        if not os.path.isdir(raw_images_path):\n",
    "            continue\n",
    "        \n",
    "        for class_name in os.listdir(raw_images_path):\n",
    "            class_input_path = os.path.join(raw_images_path, class_name)\n",
    "            class_output_path = os.path.join(output_path, class_name)\n",
    "            \n",
    "            # Zielordner neu erstellen (alte ggf. löschen)\n",
    "            if os.path.exists(class_output_path):\n",
    "                shutil.rmtree(class_output_path)\n",
    "            os.makedirs(class_output_path)\n",
    "            \n",
    "            image_count = 0\n",
    "            \n",
    "            for filename in os.listdir(class_input_path):\n",
    "                if filename.lower().endswith('.jpg'):\n",
    "                    input_path = os.path.join(class_input_path, filename)\n",
    "                    \n",
    "                    try:\n",
    "                        if processing_type == 'grayscale':\n",
    "                            # Bild in Graustufen konvertieren und skalieren\n",
    "                            img = Image.open(input_path).convert('L')\n",
    "                            img = img.resize(image_size)\n",
    "                            \n",
    "                        elif processing_type == 'wavelet':\n",
    "                            # Bild laden, auf RGB umwandeln, skalieren und Wavelet-Transformation anwenden\n",
    "                            img = cv2.imread(input_path)\n",
    "                            img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                            img_rgb = cv2.resize(img_rgb, image_size)\n",
    "                            img = w2d(img_rgb, mode=mode, level=level)\n",
    "                        \n",
    "                        # Verarbeitetes Bild speichern\n",
    "                        base_name = os.path.splitext(filename)[0]\n",
    "                        save_path = os.path.join(class_output_path, f\"{base_name}.jpg\")\n",
    "                        \n",
    "                        if processing_type == 'grayscale':\n",
    "                            img.save(save_path)\n",
    "                        else:\n",
    "                            cv2.imwrite(save_path, img)\n",
    "                        \n",
    "                        image_count += 1\n",
    "                    \n",
    "                    except Exception as e:\n",
    "                        print(f\"Fehler bei {filename}: {e}\")\n",
    "            \n",
    "            print(f\"Kategorie '{class_name}': {image_count} Bilder nach '{class_output_path}' kopiert.\")\n",
    "            total_images += image_count\n",
    "    \n",
    "    print(f\"\\nTotal kopierte Bilder: {total_images}\")\n",
    "    print(f\"Verarbeitung abgeschlossen: {processing_type.upper()}-Features gespeichert.\")\n",
    "\n",
    " # Grayscale Konvertierung:\n",
    "convert_images(processing_type='grayscale')\n",
    "\n",
    "# Wavelet Konvertierung\n",
    "convert_images(processing_type='wavelet', mode='haar', level=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Erstellen von `X` und `Y` für klassische ML-Modelle und ANN***\n",
    "\n",
    "Um klassische ML-Modelle oder ein einfaches ANN zu trainieren, müssen die Bilddaten in eine **tabellarische Form** gebracht werden.  \n",
    "Dazu werden die Bilder aus den vorbereiteten **Graustufen- oder Wavelet-Verzeichnissen** geladen, auf eine einheitliche Größe gebracht und anschließend zu **flachen Vektoren (Feature-Vektoren)** umgewandelt.\n",
    "\n",
    "- **`X_gray`** enthält die Graustufenbilder als flache Arrays(64 × 64 = 4096  Merkmale pro Bild).\n",
    "- **`X_wavelet`** enthält die Wavelet-basierten Features ebenfalls als flache Arrays.\n",
    "- **`y_full_gray`**  bzw. **`y_full_wavelet`** enthalten die zugehörigen Labelstrings in der Form: \"Tomato___Early_blight\", \"Apple___Scab\" usw.\n",
    "\n",
    "Diese strukturierte Vorbereitung ist essenziell für alle Modelle, die keine Rohbilder (wie CNNs) direkt verarbeiten können.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_flat_images_from(\n",
    "    root_dir, \n",
    "    feature_dirname='grayscale_features', \n",
    "    return_paths=False\n",
    "):\n",
    "    X = []       # Bilddaten (flach)\n",
    "    y = []       # Labels\n",
    "    paths = []   # Optional: Pfade zu den Bildern\n",
    "\n",
    "    for plant in os.listdir(root_dir):\n",
    "        plant_path = os.path.join(root_dir, plant)\n",
    "        feature_path = os.path.join(plant_path, feature_dirname)\n",
    "\n",
    "        if not os.path.isdir(feature_path):\n",
    "            continue\n",
    "\n",
    "        print(f\" Verarbeitung: {feature_path}\")\n",
    "\n",
    "        for class_name in os.listdir(feature_path):\n",
    "            class_dir = os.path.join(feature_path, class_name)\n",
    "            if not os.path.isdir(class_dir):\n",
    "                continue\n",
    "\n",
    "            for fname in os.listdir(class_dir):\n",
    "                if fname.lower().endswith('.jpg'):\n",
    "                    fpath = os.path.join(class_dir, fname)\n",
    "\n",
    "                    try:\n",
    "                        # Bild laden und in 1D-Array (flach) umwandeln\n",
    "                        img = Image.open(fpath)\n",
    "                        img_arr = np.array(img).flatten()\n",
    "                        X.append(img_arr)\n",
    "\n",
    "                        # Label aus Ordnerstruktur ableiten\n",
    "                        y.append(class_name)\n",
    "\n",
    "                        # Optional: Bildpfad speichern\n",
    "                        if return_paths:\n",
    "                            paths.append(fpath)\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\" Fehler bei {fpath}: {e}\")\n",
    "\n",
    "    # Rückgabe abhängig von return_paths\n",
    "    if return_paths:\n",
    "        return np.array(X), y, paths\n",
    "    else:\n",
    "        return np.array(X), y\n",
    "\n",
    "# Daten aus beiden Feature-Sätzen laden\n",
    "X_gray, y_full_gray, y_path_gray = load_flat_images_from(global_target_dir, feature_dirname='grayscale_features', return_paths=True)\n",
    "X_wavelet, y_full_wavelet, y_path_wavelet = load_flat_images_from(global_target_dir, feature_dirname='wavelet_features', return_paths=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Datenvorverarbeitung ist abgeschlossen. Alle Bilder wurden auf eine einheitliche Größe von 64 × 64 Pixeln gebracht, in Graustufen umgewandelt und zusätzlich einer Wavelet-Transformation unterzogen. Dadurch entstanden zwei parallele Datensätze, die als Eingabe für verschiedene Modelle genutzt werden können.\n",
    "\n",
    "- **`X_gray`** : Flache Vektoren aus Graustufenbildern,Shape: (11861, 4096)\n",
    "- **`X_wavelet`** : Flache Vektoren aus Wavelet-transformierten Bildern, Shape: (11861, 4096)\n",
    "- **`y_full_gray`** , **`y_full_wavelet`** : Labels im Format \"Pflanze___Zustand\"\n",
    "\n",
    "Zur Kontrolle wird hier ein zufällig gewählter Datensatzindex visualisiert: Das Originalbild, das Graustufenbild und das resultierende Wavelet-Bild mit der zugehörigen Beschriftung:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 42  # beliebiger Index\n",
    "fig, axs = plt.subplots(1, 3, figsize=(12,4))\n",
    "\n",
    "# Originalbild\n",
    "raw_path = y_path_gray[i].replace('grayscale_features', 'raw_images')\n",
    "img_raw = Image.open(raw_path)\n",
    "axs[0].imshow(img_raw)\n",
    "axs[0].set_title(\"Original\")\n",
    "\n",
    "# Graustufenbild\n",
    "axs[1].imshow(X_gray[i].reshape(*global_image_size), cmap='gray')\n",
    "axs[1].set_title(\"Graustufen\")\n",
    "\n",
    "# Waveletbild\n",
    "axs[2].imshow(X_wavelet[i].reshape(*global_image_size), cmap='gray')\n",
    "axs[2].set_title(\"Wavelet\")\n",
    "\n",
    "for ax in axs:\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle(y_full_gray[i])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Modellierung / Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_multiple_plants(X, Y, plant_names):\n",
    "    plant_data = {}  # Ergebnis-Dictionary: enthält pro Pflanze X und y\n",
    "\n",
    "    for plant_name in plant_names:\n",
    "        X_filtered = []     # Gefilterte Bilddaten für diese Pflanze\n",
    "        y_filtered = []     # Gefilterte Labels (nur Krankheitszustand)\n",
    "        count = 0           # Zähler für gefundene Bilder\n",
    "\n",
    "        for xi, label in zip(X, Y):\n",
    "            try:\n",
    "                parts = label.split(\"___\")\n",
    "                if len(parts) != 2:\n",
    "                    continue\n",
    "\n",
    "                plant, _ = parts\n",
    "\n",
    "                # Nur Bilder dieser Pflanze berücksichtigen\n",
    "                if plant == plant_name:\n",
    "                    X_filtered.append(xi)\n",
    "                    y_filtered.append(label)\n",
    "                    count += 1\n",
    "            except:\n",
    "                continue  # Fehlerhafte Einträge überspringen\n",
    "\n",
    "        print(f\" '{plant_name}': {count} Bilder gefunden.\")\n",
    "        plant_data[plant_name] = (np.array(X_filtered), y_filtered)\n",
    "\n",
    "    return plant_data\n",
    "\n",
    "\n",
    "plant_names = ['Apple', 'Corn', 'Tomato']\n",
    "\n",
    "# Datensätze für jede Pflanze getrennt laden\n",
    "filtered_data_gray = filter_multiple_plants(X_gray, y_full_gray, plant_names)\n",
    "filtered_data_wavelet = filter_multiple_plants(X_wavelet, y_full_wavelet, plant_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareDataToModel(filtered_data, categorie):\n",
    "    X_data, Y_data = filtered_data[categorie]\n",
    "    X = X_data\n",
    "    y = Y_data\n",
    "\n",
    "    # Label-Encoding der Zielklassen (z. B. 'Apple___healthy' → 0, ...)\n",
    "    le = LabelEncoder()\n",
    "    y_encoded = le.fit_transform(y)\n",
    "\n",
    "    ''' Aufteilen in Trainings- und Testdaten (80/20)\n",
    "        Die Daten werden automatisch geshuffelt (random_state für Reproduzierbarkeit).\n",
    "        Mit stratify=y_encoded bleibt die Klassenverteilung im Train-/Testset erhalten.\n",
    "        wichtig bei unbalancierten Klassen (z. B. 'Apple___healthy' deutlich häufiger)\n",
    "    '''\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
    "    )\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# Beispielhafte Vorbereitung der Apple-Daten (Gray + Wavelet)\n",
    "X_train_gray, X_test_gray, y_train_gray, y_test_gray = prepareDataToModel(filtered_data_gray, 'Apple')\n",
    "X_train_wavelet, X_test_wavelet, y_train_wavelet, y_test_wavelet = prepareDataToModel(filtered_data_wavelet, 'Apple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "ML-Modelle mit zugehörigen Hyperparametern für GridSearchCV:\n",
    "- SVM (linear)\n",
    "- Random Forest (Ensemble aus Decision Trees)\n",
    "- Decision Tree (einzelner Baum)\n",
    "- Logistische Regression\n",
    "\n",
    "Obwohl Random Forest auf Decision Trees basiert, wurden beide separat verglichen,\n",
    "um Einfachmodell vs. Ensemble gegenüberzustellen.\n",
    "'''\n",
    "\n",
    "model_params = {\n",
    "    'svm': {\n",
    "        'model': SVC(gamma='auto', probability=True),\n",
    "        'params': {\n",
    "            'svc__C': [0.1, 1, 10],\n",
    "            'svc__kernel': ['linear'] \n",
    "        }\n",
    "    },\n",
    "    'random_forest': {\n",
    "        'model': RandomForestClassifier(n_jobs=-1),\n",
    "        'params': {\n",
    "            'randomforestclassifier__n_estimators': [100, 200, 300],\n",
    "            'randomforestclassifier__max_features': ['sqrt', 'log2']\n",
    "        }\n",
    "    },\n",
    "    'decision_tree': {\n",
    "        'model': DecisionTreeClassifier(),\n",
    "        'params': {\n",
    "            'decisiontreeclassifier__max_depth': [10, 20, None],\n",
    "            'decisiontreeclassifier__min_samples_split': [2, 5, 10]\n",
    "        }\n",
    "    },\n",
    "    'logistic_regression': {\n",
    "        'model': LogisticRegression(solver='liblinear', multi_class='auto', max_iter=1000),\n",
    "        'params': {\n",
    "            'logisticregression__C': [0.01, 0.1, 1]\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate Klassifikation (Ansatz A)\n",
    "\n",
    "**Appfel**\n",
    "\n",
    "***Machine Lerning***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gridsearch_multiple_models(X_train, y_train,model_params,cv=3): \n",
    "    scores = []\n",
    "    best_estimators = {}\n",
    "\n",
    "    for algo, mp in model_params.items():\n",
    "        pipe = make_pipeline(StandardScaler(), mp['model'])\n",
    "        grid = GridSearchCV(pipe, mp['params'], cv=cv, n_jobs=-1,return_train_score=False)\n",
    "        grid.fit(X_train, y_train)\n",
    "        scores.append({\n",
    "            'model': algo,\n",
    "            'best_score': grid.best_score_,\n",
    "            'best_params': grid.best_params_\n",
    "        })\n",
    "        best_estimators[algo] = grid.best_estimator_\n",
    "\n",
    "    df = pd.DataFrame(scores, columns=['model', 'best_score', 'best_params'])\n",
    "    return df, best_estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die `X_gray`-Daten wurden zur Modelloptimierung verwendet. Das leistungsstärkste Modell (basierend auf Cross-Validation) wurde ermittelt und anschließend zur späteren Wiederverwendung als `.pkl`-Datei gespeichert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Speicherung des trainierten Modells (klassisch oder Deep Learning)\n",
    "def save_model(model, path_directory, file_name, model_type='ml'):\n",
    "    if not os.path.exists(path_directory):\n",
    "        os.makedirs(path_directory)\n",
    "    \n",
    "    save_path = os.path.join(path_directory, file_name)\n",
    "    \n",
    "    if model_type == 'ml':\n",
    "        joblib.dump(model, save_path)\n",
    "    elif model_type == 'dl':\n",
    "        model.save(save_path)\n",
    "    else:\n",
    "        raise ValueError(\"Unbekannter Modelltyp: 'ml' (scikit-learn) oder 'dl' (Deep Learning/Keras) erwartet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML-Model Trainieren\n",
    "df, estimators = gridsearch_multiple_models(X_train_gray, y_train_gray, model_params)\n",
    "\n",
    "# Bestes Modell wählen\n",
    "best_model_name = df.loc[df['best_score'].idxmax()]['model']\n",
    "best_model = estimators[best_model_name]\n",
    "\n",
    "# Trainiertes Modell speichern\n",
    "save_model(best_model,'saved_models/v1/ml/apple','best_model_apple_gray_new.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML-Model Trainieren\n",
    "df, estimators = gridsearch_multiple_models(X_train_wavelet, y_train_wavelet,model_params)\n",
    "\n",
    "# Bestes Modell wählen\n",
    "best_model_name = df.loc[df['best_score'].idxmax()]['model']\n",
    "best_model = estimators[best_model_name]\n",
    "\n",
    "# Speichern\n",
    "save_model(best_model,'saved_models/v1/ml/apple','best_model_apple_wavelet.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Künstliches Neuronales Netz (ANN)***\n",
    "\n",
    "Für jede Pflanzenkategorie wurde ein Artificial Neural Network (ANN) erstellt, das auf flach dargestellten Bildmerkmalen basiert (`X_gray`, `X_wavelet`).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimal_batch_size(dataset_size, model_type='ann'):\n",
    "\n",
    "    if model_type.lower() == 'cnn':\n",
    "        if dataset_size <= 3000:\n",
    "            return 32\n",
    "        elif dataset_size <= 6000:\n",
    "            return 48\n",
    "        elif dataset_size <= 9000:\n",
    "            return 64\n",
    "        else: \n",
    "            return 80\n",
    "    \n",
    "    else: \n",
    "        if dataset_size <= 3000:\n",
    "            return 48\n",
    "        elif dataset_size <= 6000:\n",
    "            return 64\n",
    "        elif dataset_size <= 9000:\n",
    "            return 96\n",
    "        else:\n",
    "            return 128\n",
    "\n",
    "def train_model(model, X_train, y_train, X_test, y_test, model_type='ann'):\n",
    "    \n",
    "    # Ermittele die optimale Batch-Größe\n",
    "    dataset_size = len(X_train)\n",
    "    optimal_batch_size = get_optimal_batch_size(dataset_size, model_type)\n",
    "    \n",
    "    print(f\"Datensatzgröße: {dataset_size}\")\n",
    "    print(f\"Modelltyp: {model_type.upper()}\")\n",
    "    print(f\"Optimale Batch-Größe: {optimal_batch_size}\")\n",
    "    \n",
    "    callbacks = [\n",
    "        EarlyStopping(\n",
    "            patience=15,  \n",
    "            restore_best_weights=True,\n",
    "            verbose=1,\n",
    "            monitor='val_accuracy' \n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            factor=0.7, \n",
    "            patience=8,\n",
    "            min_lr=1e-6,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Modell kompilieren\n",
    "    model.compile(\n",
    "        optimizer=AdamW(learning_rate=0.0005),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=150, \n",
    "        batch_size=optimal_batch_size, \n",
    "        validation_data=(X_test, y_test),\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    return history\n",
    "\n",
    "def create_ann(output=3):\n",
    "    \"\"\"ANN model architecture\"\"\"\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=(global_image_size[0]*global_image_size[1],)),\n",
    "        \n",
    "        # Mehr Kapazität für komplexe Muster\n",
    "        layers.Dense(1024, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.4),\n",
    "        \n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.3),\n",
    "        \n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.2),\n",
    "        \n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dropout(0.1),\n",
    "        \n",
    "        layers.Dense(output, activation='softmax')\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_ann_adaptive(X, y, create_model_fn, model_type='ann', cv_folds=3, random_state=42):\n",
    "\n",
    "    # KFold-Cross-Validation mit Shuffling und fixiertem random_state für Reproduzierbarkeit\n",
    "    kf = KFold(n_splits=cv_folds, shuffle=True, random_state=random_state)\n",
    "    val_accuracies = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
    "        print(f\"\\n Fold {fold+1}/{cv_folds}\")\n",
    "\n",
    "        # Trainings- und Validierungsdaten für aktuellen Fold aufteilen\n",
    "        X_train_fold, X_val_fold = X[train_idx], X[val_idx]\n",
    "        y_train_fold, y_val_fold = y[train_idx], y[val_idx]\n",
    "\n",
    "        # Feature-Skalierung (only for ANN, CNNs typically don't need it)\n",
    "        if model_type.lower() == 'ann':\n",
    "            scaler = StandardScaler()\n",
    "            X_train_fold_scaled = scaler.fit_transform(X_train_fold)\n",
    "            X_val_fold_scaled = scaler.transform(X_val_fold)\n",
    "        else:\n",
    "            X_train_fold_scaled = X_train_fold\n",
    "            X_val_fold_scaled = X_val_fold\n",
    "\n",
    "        # Neues Modell erstellen\n",
    "        model = create_model_fn()\n",
    "\n",
    "        # Modelltraining mit adaptiver Batch-Größe\n",
    "        train_model(\n",
    "            model,\n",
    "            X_train_fold_scaled, y_train_fold,\n",
    "            X_val_fold_scaled, y_val_fold,\n",
    "            model_type=model_type\n",
    "        )\n",
    "\n",
    "        # Validierungsgenauigkeit messen\n",
    "        val_loss, val_accuracy = model.evaluate(X_val_fold_scaled, y_val_fold, verbose=0)\n",
    "        print(f\" Fold-{fold+1} Validierungs-Genauigkeit: {val_accuracy:.4f}\")\n",
    "        val_accuracies.append(val_accuracy)\n",
    "\n",
    "    # Durchschnittliche Genauigkeit über alle Folds\n",
    "    mean_val_acc = np.mean(val_accuracies)\n",
    "    print(f\"\\n Durchschnittliche CV-Genauigkeit über {cv_folds} Folds: {mean_val_acc:.4f}\")\n",
    "\n",
    "    return mean_val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Cross-Validation Gray Data*** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cross-Validation für das ANN-Modell mit 3 Folds\n",
    "mean_accuracy = cross_validate_ann_adaptive(\n",
    "    X_train_gray, y_train_gray,\n",
    "    create_model_fn=create_ann,\n",
    "    model_type='ann', \n",
    "    cv_folds=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Funktion zur Skalierung von Trainings- und Testdaten\n",
    "def scale_data(X_train, X_test):\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    return X_train_scaled, X_test_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daten skalieren (Standardisierung)\n",
    "X_train_gray_scaled, X_test_gray_scaled = scale_data(X_train_gray, X_test_gray)\n",
    "\n",
    "# Löscht vorherige Modelle aus dem Speicher und verhindert Speicherüberlauf bei mehrfacher Modellerstellung\n",
    "K.clear_session()\n",
    "\n",
    "# ANN-Modell erstellen und trainieren\n",
    "model = create_ann() \n",
    "history = train_model(model, X_train_gray_scaled, y_train_gray, X_test_gray_scaled, y_test_gray)\n",
    "\n",
    "# Trainiertes Modell speichern (Deep Learning Format)\n",
    "save_model(model, 'saved_models/v1/ann/apple', 'model_apple_gray.keras', 'dl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Cross-Validation Wavelet Data*** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_accuracy = cross_validate_ann_adaptive(\n",
    "    X_train_wavelet, y_train_wavelet,\n",
    "    create_model_fn=create_ann,\n",
    "    model_type='ann', \n",
    "    cv_folds=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daten skalieren (Standardisierung)\n",
    "X_train_wavelet_scaled,X_test_wavelet_scaled = scale_data(X_train_wavelet, X_test_wavelet)\n",
    "\n",
    "# Löscht vorherige Modelle aus dem Speicher\n",
    "K.clear_session()\n",
    "\n",
    "# Modellinitialisierung und Training\n",
    "model = create_ann() \n",
    "history = train_model(model, X_train_wavelet_scaled, y_train_wavelet, X_test_wavelet_scaled, y_test_wavelet)\n",
    "    \n",
    "# Speicherung des trainierten Modells\n",
    "save_model(model,'saved_models/v1/ann/apple','model_apple_wavelet.keras','dl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Faltungsneuronales Netz (CNN)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datenvorbereitung für CNN\n",
    "def prepare_cnn_input(X_train, X_test, image_size):\n",
    "    X_train_cnn = X_train.reshape(-1, image_size[0], image_size[1], 1) / 255.0\n",
    "    X_test_cnn = X_test.reshape(-1, image_size[0], image_size[1], 1) / 255.0\n",
    "    return X_train_cnn, X_test_cnn\n",
    "\n",
    "# CNN-Eingabe: Von flachen Vektoren zu 4D-Tensoren (und normalisieren)\n",
    "X_train_gray_cnn, X_test_gray_cnn = prepare_cnn_input(X_train_gray, X_test_gray, global_image_size)\n",
    "X_train_wavelet_cnn, X_test_wavelet_cnn = prepare_cnn_input(X_train_wavelet, X_test_wavelet, global_image_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_model(output_base=3, use_augmentation=False):\n",
    "    model = models.Sequential()\n",
    "\n",
    "\n",
    "    model.add(layers.Input(shape=(global_image_size[0], global_image_size[1], 1)))\n",
    "\n",
    "    if use_augmentation:\n",
    "        data_augmentation = tf.keras.Sequential([\n",
    "            layers.RandomFlip(\"horizontal\"),\n",
    "            layers.RandomRotation(0.1),\n",
    "            layers.RandomZoom(0.1),\n",
    "            layers.RandomContrast(0.1)\n",
    "        ])\n",
    "        model.add(data_augmentation)\n",
    "\n",
    "    # Convolutional Layers\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "\n",
    "    model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "\n",
    "    model.add(layers.Conv2D(256, (3, 3), activation='relu'))\n",
    "    model.add(layers.GlobalAveragePooling2D())\n",
    "\n",
    "    # Dense Layers\n",
    "    model.add(layers.Dense(512, activation='relu'))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    model.add(layers.Dense(256, activation='relu'))\n",
    "    model.add(layers.Dropout(0.2))\n",
    "    model.add(layers.Dense(output_base, activation='softmax'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das CNN-Modell besteht aus vier Faltungsblöcken mit jeweils Conv2D, MaxPooling und Batch-Normalisierung, gefolgt von dichten Schichten mit Dropout zur Vermeidung von Overfitting und endet mit einer Softmax-Ausgabe für die dreiklassige Klassifikation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "***Training des CNN-Modells ohne Augmentierung***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modellinitialisierung und Training\n",
    "model = create_cnn_model() \n",
    "history = train_model(model, X_train_gray_cnn, y_train_gray, X_test_gray_cnn, y_test_gray,model_type='cnn')\n",
    "\n",
    "# Löscht vorherige Modelle aus dem Speicher\n",
    "K.clear_session()\n",
    "\n",
    "# Speicherung des trainierten Modells\n",
    "save_model(model,'saved_models/v1/cnn/apple','model_without_augmentation_apple_gray.keras','dl')\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "# Modellinitialisierung und Training\n",
    "model = create_cnn_model() \n",
    "history = train_model(model, X_train_wavelet_cnn, y_train_wavelet, X_test_wavelet_cnn, y_test_wavelet,model_type='cnn')\n",
    "\n",
    "# Speicherung des trainierten Modells\n",
    "save_model(model,'saved_models/v1/cnn/apple','model_without_augmentation_apple_wavelet.keras','dl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Training des CNN-Modells mit Augmentierung***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "\n",
    "# Modellinitialisierung und Training\n",
    "model = create_cnn_model(output_base=3, use_augmentation=True)\n",
    "history = train_model(model, X_train_gray_cnn, y_train_gray, X_test_gray_cnn, y_test_gray,model_type='cnn')\n",
    "\n",
    "# Speicherung des trainierten Modells\n",
    "save_model(model,'saved_models/v1/cnn/apple','model_with_augmentation_apple_gray.keras','dl')\n",
    "K.clear_session()\n",
    "\n",
    "# Modellinitialisierung und Training\n",
    "model = create_cnn_model(output_base=3, use_augmentation=True)\n",
    "history = train_model(model, X_train_wavelet_cnn, y_train_wavelet, X_test_wavelet_cnn, y_test_wavelet,model_type='cnn')\n",
    "\n",
    "# Speicherung des trainierten Modells\n",
    "save_model(model,'saved_models/v1/cnn/apple','model_with_augmentation_apple_wavelet.keras','dl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tomaten**\n",
    "\n",
    "***Machine Learning***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data für Tomaten vorbereiten\n",
    "X_train_gray, X_test_gray, y_train_gray, y_test_gray=prepareDataToModel(filtered_data_gray, 'Tomato')  \n",
    "X_train_wavelet, X_test_wavelet, y_train_wavelet, y_test_wavelet=prepareDataToModel(filtered_data_wavelet, 'Tomato') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML-Model Trainieren\n",
    "df, estimators = gridsearch_multiple_models(X_train_gray, y_train_gray, model_params)\n",
    "\n",
    "# Bestes Modell wählen\n",
    "best_model_name = df.loc[df['best_score'].idxmax()]['model']\n",
    "best_model = estimators[best_model_name]\n",
    "\n",
    "# Speicherung des trainierten Modells\n",
    "save_model(best_model, f'saved_models/v1/ml/tomato','best_model_tomato_gray.pkl')\n",
    "\n",
    "# ML-Model Trainieren\n",
    "df, estimators = gridsearch_multiple_models(X_train_wavelet, y_train_wavelet,model_params)\n",
    "\n",
    "# Bestes Modell wählen\n",
    "best_model_name = df.loc[df['best_score'].idxmax()]['model']\n",
    "best_model = estimators[best_model_name]\n",
    "\n",
    "# Speicherung des trainierten Modells\n",
    "save_model(best_model, f'saved_models/v1/ml/tomato','best_model_tomato_wavelet.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Künstliches Neuronales Netz (ANN)***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Cross Validation Gray Data***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-Validation für das ANN-Modell mit 3 Folds\n",
    "mean_accuracy = cross_validate_ann_adaptive(\n",
    "    X_train_gray, y_train_gray,\n",
    "    create_model_fn=create_ann,\n",
    "    model_type='ann', \n",
    "    cv_folds=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daten skalieren (Standardisierung)\n",
    "X_train_gray_scaled, X_test_gray_scaled  = scale_data(X_train_gray, X_test_gray)\n",
    "K.clear_session()\n",
    "\n",
    "# Modellinitialisierung und Training\n",
    "model = create_ann() \n",
    "history = train_model(model, X_train_gray_scaled, y_train_gray, X_test_gray_scaled, y_test_gray)\n",
    "\n",
    "# Speicherung des trainierten Modells\n",
    "save_model(model,'saved_models/v1/ann/tomato','model_tomato_gray.keras','dl')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Cross Validation Wavelet Data***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_accuracy = cross_validate_ann_adaptive(\n",
    "    X_train_wavelet, y_train_wavelet,\n",
    "    create_model_fn=create_ann,\n",
    "    model_type='ann', \n",
    "    cv_folds=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daten skalieren (Standardisierung)\n",
    "X_train_wavelet_scaled,X_test_wavelet_scaled = scale_data(X_train_wavelet, X_test_wavelet)\n",
    "K.clear_session()\n",
    "\n",
    "# Modellinitialisierung und Training\n",
    "model = create_ann() \n",
    "history = train_model(model, X_train_wavelet_scaled, y_train_wavelet, X_test_wavelet_scaled, y_test_wavelet)\n",
    "\n",
    "# Speicherung des trainierten Modells\n",
    "save_model(model,'saved_models/v1/ann/tomato','model_tomato_wavelet.keras','dl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Faltungsneuronales Netz (CNN)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN-Eingabe: Von flachen Vektoren zu 4D-Tensoren (und normalisieren)\n",
    "X_train_gray_cnn, X_test_gray_cnn = prepare_cnn_input(X_train_gray, X_test_gray, global_image_size)\n",
    "X_train_wavelet_cnn, X_test_wavelet_cnn = prepare_cnn_input(X_train_wavelet, X_test_wavelet, global_image_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Training des CNN-Modells ohne Augmentierung***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "\n",
    "# Modellinitialisierung und Training\n",
    "model = create_cnn_model() \n",
    "history = train_model(model, X_train_gray_cnn, y_train_gray, X_test_gray_cnn, y_test_gray,model_type='cnn')\n",
    "\n",
    "# Speicherung des trainierten Modells\n",
    "save_model(model,'saved_models/v1/cnn/tomato','model_without_augmentation_tomato_gray.keras','dl')\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "# Modellinitialisierung und Training\n",
    "model = create_cnn_model() \n",
    "history = train_model(model, X_train_wavelet_cnn, y_train_wavelet, X_test_wavelet_cnn, y_test_wavelet,model_type='cnn')\n",
    "\n",
    "# Speicherung des trainierten Modells\n",
    "save_model(model,'saved_models/v1/cnn/tomato','model_without_augmentation_tomato_wavelet.keras','dl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Training des CNN-Modells mit Augmentierung***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "\n",
    "# Modellinitialisierung und Training\n",
    "model = create_cnn_model(output_base=3, use_augmentation=True)\n",
    "history = train_model(model, X_train_gray_cnn, y_train_gray, X_test_gray_cnn, y_test_gray,model_type='cnn')\n",
    "\n",
    "# Speicherung des trainierten Modells\n",
    "save_model(model,'saved_models/v1/cnn/tomato','model_with_augmentation_tomato_gray.keras','dl')\n",
    "K.clear_session()\n",
    "\n",
    "# Modellinitialisierung und Training\n",
    "model = create_cnn_model(output_base=3, use_augmentation=True)\n",
    "history = train_model(model, X_train_wavelet_cnn, y_train_wavelet, X_test_wavelet_cnn, y_test_wavelet,model_type='cnn')\n",
    "\n",
    "# Speicherung des trainierten Modells\n",
    "save_model(model,'saved_models/v1/cnn/tomato','model_with_augmentation_tomato_wavelet.keras','dl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mais**\n",
    "\n",
    "***Machine Learning***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_gray, X_test_gray, y_train_gray, y_test_gray=prepareDataToModel(filtered_data_gray, 'Corn')  \n",
    "X_train_wavelet, X_test_wavelet, y_train_wavelet, y_test_wavelet=prepareDataToModel(filtered_data_wavelet, 'Corn') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML-Model Trainieren\n",
    "df, estimators = gridsearch_multiple_models(X_train_gray, y_train_gray, model_params)\n",
    "\n",
    "# Bestes Modell wählen\n",
    "best_model_name = df.loc[df['best_score'].idxmax()]['model']\n",
    "best_model = estimators[best_model_name]\n",
    "\n",
    "# Speicherung des trainierten Modells\n",
    "save_model(best_model, f'saved_models/v1/ml/corn','best_model_corn_gray.pkl')\n",
    "\n",
    "# ML-Model Trainieren\n",
    "df, estimators = gridsearch_multiple_models(X_train_wavelet, y_train_wavelet,model_params)\n",
    "\n",
    "# Bestes Modell wählen\n",
    "best_model_name = df.loc[df['best_score'].idxmax()]['model']\n",
    "print(best_model_name)\n",
    "best_model = estimators[best_model_name]\n",
    "\n",
    "# Speicherung des trainierten Modells\n",
    "save_model(best_model, f'saved_models/v1/ml/corn','best_model_corn_wavelet.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Künstliches Neuronales Netz (ANN)***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Cross Validation Gray Data***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_accuracy = cross_validate_ann_adaptive(\n",
    "    X_train_gray, y_train_gray,\n",
    "    create_model_fn=create_ann,\n",
    "    model_type='ann', \n",
    "    cv_folds=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daten skalieren (Standardisierung)\n",
    "X_train_gray_scaled, X_test_gray_scaled  = scale_data(X_train_gray, X_test_gray)\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "# Modellinitialisierung und Training\n",
    "model = create_ann() \n",
    "history = train_model(model, X_train_gray_scaled, y_train_gray, X_test_gray_scaled, y_test_gray)\n",
    "    \n",
    "# Speicherung des trainierten Modells\n",
    "save_model(model,'saved_models/v1/ann/corn','model_corn_gray.keras','dl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Cross Validation Wavelet Data***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_accuracy = cross_validate_ann_adaptive(\n",
    "    X_train_wavelet, y_train_wavelet,\n",
    "    create_model_fn=create_ann,\n",
    "    model_type='ann', \n",
    "    cv_folds=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daten skalieren (Standardisierung)\n",
    "X_train_wavelet_scaled,X_test_wavelet_scaled = scale_data(X_train_wavelet, X_test_wavelet)\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "# Modellinitialisierung und Training\n",
    "model = create_ann() \n",
    "history = train_model(model, X_train_wavelet_scaled, y_train_wavelet, X_test_wavelet_scaled, y_test_wavelet)\n",
    "    \n",
    "# Speicherung des trainierten Modells\n",
    "save_model(model,'saved_models/v1/ann/corn','model_corn_wavelet.keras','dl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Training des CNN-Modells ohne Augmentierung***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data vorbereiten\n",
    "X_train_gray_cnn, X_test_gray_cnn = prepare_cnn_input(X_train_gray, X_test_gray, global_image_size)\n",
    "X_train_wavelet_cnn, X_test_wavelet_cnn = prepare_cnn_input(X_train_wavelet, X_test_wavelet, global_image_size)\n",
    "\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "# Modellinitialisierung und Training\n",
    "model = create_cnn_model() \n",
    "history = train_model(model, X_train_gray_cnn, y_train_gray, X_test_gray_cnn, y_test_gray,model_type='cnn')\n",
    "\n",
    "# Speicherung des trainierten Modells\n",
    "save_model(model,'saved_models/v1/cnn/corn','model_without_augmentation_corn_gray.keras','dl')\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "# Modellinitialisierung und Training\n",
    "model = create_cnn_model() \n",
    "history = train_model(model, X_train_wavelet_cnn, y_train_wavelet, X_test_wavelet_cnn, y_test_wavelet,model_type='cnn')\n",
    "\n",
    "# Speicherung des trainierten Modells\n",
    "save_model(model,'saved_models/v1/cnn/corn','model_without_augmentation_corn_wavelet.keras','dl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Training des CNN-Modells mit Augmentierung***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "\n",
    "# Modellinitialisierung und Training\n",
    "model = create_cnn_model(output_base=3, use_augmentation=True)\n",
    "history = train_model(model, X_train_gray_cnn, y_train_gray, X_test_gray_cnn, y_test_gray,model_type='cnn')\n",
    "\n",
    "# Speicherung des trainierten Modells\n",
    "save_model(model,'saved_models/v1/cnn/corn','model_with_augmentation_corn_gray.keras','dl')\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "# Modellinitialisierung und Training\n",
    "model = create_cnn_model(output_base=3, use_augmentation=True)\n",
    "history = train_model(model, X_train_wavelet_cnn, y_train_wavelet, X_test_wavelet_cnn, y_test_wavelet,model_type='cnn')\n",
    "\n",
    "# Speicherung des trainierten Modells\n",
    "save_model(model,'saved_models/v1/cnn/corn','model_with_augmentation_corn_wavelet.keras','dl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kombinierte Klassifikation (Ansatz B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_combined_data(X, y_full):\n",
    "    le = LabelEncoder()\n",
    "    y_encoded = le.fit_transform(y_full)\n",
    "\n",
    "    ''' Aufteilen in Trainings- und Testdaten (80/20)\n",
    "        Die Daten werden automatisch geshuffelt (random_state für Reproduzierbarkeit).\n",
    "        Mit stratify=y_encoded bleibt die Klassenverteilung im Train-/Testset erhalten.\n",
    "        wichtig bei unbalancierten Klassen (z.B. 'Apple___healthy' deutlich häufiger)\n",
    "    '''\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded )\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "X_train_gray, X_test_gray, y_train_gray, y_test_gray = prepare_combined_data(X_gray, y_full_gray)\n",
    "X_train_wavelet, X_test_wavelet, y_train_wavelet, y_test_wavelet = prepare_combined_data(X_wavelet, y_full_wavelet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    'svm': {\n",
    "        'model': SVC(gamma='auto', probability=True),\n",
    "        'params': {\n",
    "            'svc__C': [1],\n",
    "            'svc__kernel': ['linear']  \n",
    "        }\n",
    "    },\n",
    "    'random_forest': {\n",
    "        'model': RandomForestClassifier(n_jobs=-1),\n",
    "        'params': {\n",
    "            'randomforestclassifier__n_estimators': [100], \n",
    "            'randomforestclassifier__max_features': ['sqrt']\n",
    "        }\n",
    "    },\n",
    "    'decision_tree': {\n",
    "        'model': DecisionTreeClassifier(),\n",
    "        'params': {\n",
    "            'decisiontreeclassifier__max_depth': [10],\n",
    "            'decisiontreeclassifier__min_samples_split': [2]\n",
    "        }\n",
    "    },\n",
    "    'logistic_regression': {\n",
    "        'model': LogisticRegression(solver='liblinear', multi_class='auto', max_iter=1000),\n",
    "        'params': {\n",
    "            'logisticregression__C': [0.1]\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Machine Learning***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML-Model Trainieren\n",
    "df, estimators = gridsearch_multiple_models(X_train_gray, y_train_gray, model_params,2)\n",
    "\n",
    "# Bestes Modell wählen \n",
    "best_model_name = df.loc[df['best_score'].idxmax()]['model']\n",
    "best_model = estimators[best_model_name]\n",
    "\n",
    "# Speicherung des trainierten Modells\n",
    "save_model(best_model,'saved_models/v2/ml','best_model_gray.pkl')\n",
    "\n",
    "df, estimators = gridsearch_multiple_models(X_train_wavelet, y_train_wavelet, model_params,2)\n",
    "\n",
    "# Bestes Modell wählen\n",
    "best_model_name = df.loc[df['best_score'].idxmax()]['model']\n",
    "best_model = estimators[best_model_name]\n",
    "\n",
    "# Speicherung des trainierten Modells\n",
    "save_model(best_model,'saved_models/v2/ml','best_model_wavelet.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Künstliches Neuronales Netz (ANN)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skalierung, Modellinitialisierung und Training\n",
    "X_train_gray_scaled,X_test_gray_scaled = scale_data(X_train_gray,X_test_gray)\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "# Modellinitialisierung und Training\n",
    "model = create_ann(9) \n",
    "history = train_model(model, X_train_gray_scaled, y_train_gray, X_test_gray_scaled, y_test_gray)\n",
    "\n",
    "# Speicherung des trainierten Modells\n",
    "save_model(model,'saved_models/v2/ann','model_gray.keras','dl')\n",
    "\n",
    "# Skalierung, Modellinitialisierung und Training\n",
    "X_train_wavelet_scaled , X_test_wavelet_scaled= scale_data(X_train_wavelet,X_test_wavelet)\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "# Modellinitialisierung und Training\n",
    "model = create_ann(9) \n",
    "history = train_model(model, X_train_wavelet_scaled, y_train_wavelet, X_test_wavelet_scaled, y_test_wavelet)\n",
    "\n",
    "# Speicherung des trainierten Modells\n",
    "save_model(model,'saved_models/v2/ann','model_wavelet.keras','dl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Faltungsneuronales Netz (CNN)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN-Eingabe: Von flachen Vektoren zu 4D-Tensoren (und normalisieren)\n",
    "X_train_gray_cnn, X_test_gray_cnn = prepare_cnn_input(X_train_gray, X_test_gray, global_image_size)\n",
    "X_train_wavelet_cnn, X_test_wavelet_cnn = prepare_cnn_input(X_train_wavelet, X_test_wavelet, global_image_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Training des CNN-Modells ohne Augmentierung***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "\n",
    "# Modellinitialisierung und Training\n",
    "model = create_cnn_model(9) \n",
    "history = train_model(model, X_train_gray_cnn, y_train_gray, X_test_gray_cnn, y_test_gray,model_type='cnn')\n",
    "\n",
    "# Speicherung des trainierten Modells\n",
    "save_model(model,'saved_models/v2/cnn','model_without_augmentation_gray.keras','dl')\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "# Modellinitialisierung und Training\n",
    "model = create_cnn_model(9) \n",
    "history = train_model(model, X_train_wavelet_cnn, y_train_wavelet, X_test_wavelet_cnn, y_test_wavelet,model_type='cnn')\n",
    "\n",
    "# Speicherung des trainierten Modells\n",
    "save_model(model,'saved_models/v2/cnn','model_without_augmentation_wavelet.keras','dl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Training des CNN-Modells mit Augmentierung***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "\n",
    "# Modellinitialisierung und Training\n",
    "model = create_cnn_model(9,use_augmentation=True) \n",
    "history = train_model(model, X_train_gray_cnn, y_train_gray, X_test_gray_cnn, y_test_gray,model_type='cnn')\n",
    "\n",
    "# Speicherung des trainierten Modells\n",
    "save_model(model,'saved_models/v2/cnn','model_with_augmentation_gray.keras','dl')\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "# Modellinitialisierung und Training\n",
    "model = create_cnn_model(9,use_augmentation=True) \n",
    "history = train_model(model, X_train_wavelet_cnn, y_train_wavelet, X_test_wavelet_cnn, y_test_wavelet,model_type='cnn')\n",
    "\n",
    "# Speicherung des trainierten Modells\n",
    "save_model(model,'saved_models/v2/cnn','model_with_augmentation_wavelet.keras','dl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation und Vergleich\n",
    "\n",
    "### 4.1 Bewertungsmethoden\n",
    "\n",
    "Zur Bewertung der Klassifikationsmodelle werden folgende Metriken verwendet:\n",
    "\n",
    "- **Accuracy**: Anteil korrekt vorhergesagter Klassen.\n",
    "- **Confusion Matrix**: Detaillierte Übersicht über falsch und richtig klassifizierte Klassen.\n",
    "- **F1-Score**, **Precision** und **Recall**: Ergänzende Kennzahlen für die differenzierte Bewertung bei mehrklassigen oder unausgewogenen Datensätzen.\n",
    "\n",
    "***Ergebnisse – Ansatz A: Separate Klassifikation***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "def evaluate_model(model,model_type, category_name,data_Variant, X_test, y_test):\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    if y_pred.ndim > 1 and y_pred.shape[1] > 1:\n",
    "        y_pred = np.argmax(y_pred, axis=1)\n",
    "    \n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    \n",
    "    result = {\n",
    "        category_name: {\n",
    "        'Model': model_type,\n",
    "        'Category': category_name,  \n",
    "        'Data_Variant': data_Variant,  \n",
    "        'Accuracy': round(acc, 4),\n",
    "        'Precision': round(report['macro avg']['precision'], 4),\n",
    "        'Recall': round(report['macro avg']['recall'], 4),\n",
    "        'F1-Score': round(report['macro avg']['f1-score'], 4),\n",
    "        }\n",
    "    }\n",
    "    return result[category_name]\n",
    "def evualateAllModels(categories, data_variants, model_types, version='v1'):\n",
    "    results = []\n",
    "\n",
    "    for category in categories:\n",
    "        for model_type in model_types:\n",
    "            for variant in data_variants:\n",
    "                print(f\"{model_type.upper()} | {category} | {variant} | Version: {version}\")\n",
    "\n",
    "                base_path = f\"saved_models/{version}/{model_type}/{category.lower()}/\"\n",
    "                \n",
    "                models_to_evaluate = []\n",
    "\n",
    "                # ML-Modelle laden\n",
    "                if model_type == 'ml':\n",
    "                    model_path =   f\"{base_path}best_model_{category}_{variant}.pkl\"\n",
    "        \n",
    "                    try:\n",
    "                        model = joblib.load(model_path)\n",
    "                        models_to_evaluate.append((f\"ML - {variant}\", model))\n",
    "                    except Exception as e:\n",
    "                        print(f\"ML-Modell nicht gefunden: {model_path}\")\n",
    "\n",
    "                elif model_type == 'ann':\n",
    "                    model_path =      f\"{base_path}model_{category}_{variant}.keras\"\n",
    "                    try:\n",
    "                        model = load_model(model_path)\n",
    "                        models_to_evaluate.append((f\"ANN - {variant}\", model))\n",
    "                    except Exception as e:\n",
    "                        print(f\"ANN-Modell nicht gefunden: {model_path}\")\n",
    "\n",
    "                # CNN-Modelle laden (mit & ohne Augmentierung)\n",
    "                elif model_type == 'cnn':\n",
    "                  \n",
    "                        model_path_no_aug = f\"{base_path}model_without_augmentation_{category}_{variant}.keras\"\n",
    "                        try:\n",
    "                            model = load_model(model_path_no_aug)\n",
    "                            models_to_evaluate.append((f\"CNN (ohne Aug.) - {variant}\", model))\n",
    "                        except Exception as e:\n",
    "                             print(f\" Kein Modell ohne Augmentierung gefunden: {model_path_no_aug}\")\n",
    "\n",
    "                        model_path_aug = f\"{base_path}model_with_augmentation_{category}_{variant}.keras\"\n",
    "                        try:\n",
    "                            model = load_model(model_path_aug)\n",
    "                            models_to_evaluate.append((f\"CNN (mit Aug.) - {variant}\", model))\n",
    "                        except Exception as e:\n",
    "                           print(f\" Kein Modell mit Augmentierung gefunden: {model_path_aug}\")\n",
    "\n",
    "                # Daten vorbereiten\n",
    "                if variant == 'gray':\n",
    "                    X_train, X_test, _, y_test = prepareDataToModel(filtered_data_gray, category.capitalize())\n",
    "                else:\n",
    "                    X_train, X_test, _, y_test = prepareDataToModel(filtered_data_wavelet, category.capitalize())\n",
    "\n",
    "                # ANN: Skalieren\n",
    "                if model_type == 'ann':\n",
    "                    scaler = StandardScaler()\n",
    "                    X_train = scaler.fit_transform(X_train)\n",
    "                    X_test = scaler.transform(X_test)\n",
    "\n",
    "                # CNN: Reshape & Normalisieren\n",
    "                if model_type == 'cnn':\n",
    "                    X_test = X_test.reshape(-1, global_image_size[0], global_image_size[1], 1) / 255.0\n",
    "\n",
    "                # Modelle evaluieren\n",
    "                for model_label, model in models_to_evaluate:\n",
    "                    result = evaluate_model(model, model_label, category, variant.capitalize(), X_test, y_test)\n",
    "                    results.append(result)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['apple','tomato','corn']\n",
    "data_variants = ['gray', 'wavelet']\n",
    "model_types = ['ml', 'ann','cnn']\n",
    "\n",
    "result=evualateAllModels(categories,data_variants,model_types,'v1')\n",
    "\n",
    "df_results_v1 = pd.DataFrame(result)\n",
    "\n",
    "pd.set_option('display.precision', 4)\n",
    "df_results_v1.style.set_caption(\"Tabelle 4.1: Vergleich der Modelle für Variante A\") \\\n",
    "                .set_table_styles([{\n",
    "                    'selector': 'caption',\n",
    "                    'props': [('color', 'red'), ('font-size', '15px')]\n",
    "                }])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Ergebnisse – Ansatz B: Kombinierte Klassifikation**\n",
    "\n",
    "- ***Confusion Matrix***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daten vorbereiten\n",
    "X_train, X_test, y_train, y_test = prepare_combined_data(X_gray, y_full_gray)\n",
    "X_test = X_test.reshape(-1, global_image_size[0], global_image_size[1], 1) / 255.0\n",
    "\n",
    "# Vorgefertigtes Modell laden (hier CNN ohne Datenaugmentation)\n",
    "model_path='saved_models/v2/cnn/model_without_augmentation_gray.keras'\n",
    "model = load_model(model_path)\n",
    "\n",
    "# Modellvorhersagen als Wahrscheinlichkeiten berechnen\n",
    "y_pred_proba = model.predict(X_test)\n",
    "\n",
    "# Für jede Probe die Klasse mit der höchsten Wahrscheinlichkeit auswählen\n",
    "y_pred = y_pred_proba.argmax(axis=1)\n",
    "\n",
    "# Klassenliste erstellen, sortiert und eindeutig, für Achsenbeschriftung der Confusion Matrix\n",
    "labels = sorted(list(set(y_test)))\n",
    "\n",
    "# Confusion Matrix berechnen\n",
    "cm = confusion_matrix(y_test, y_pred, labels=labels)\n",
    "\n",
    "print(cm)\n",
    "\n",
    "# Confusion Matrix visualisieren\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "disp.plot(ax=ax, cmap=\"Blues\", xticks_rotation=45)\n",
    "plt.title(\"Confusion Matrix – Variante B (CNN)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Ergebnisse – Ansatz B: Separate Klassifikation***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, model_type, category_name, data_variant, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Falls Wahrscheinlichkeiten, in Klassen umwandeln\n",
    "    if y_pred.ndim > 1 and y_pred.shape[1] > 1:\n",
    "        y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "    # Falls y_test One-Hot, in Klassenlabels umwandeln\n",
    "    if y_test.ndim > 1 and y_test.shape[1] > 1:\n",
    "        y_test_labels = np.argmax(y_test, axis=1)\n",
    "    else:\n",
    "        y_test_labels = y_test\n",
    "\n",
    "    acc = accuracy_score(y_test_labels, y_pred)\n",
    "    report = classification_report(y_test_labels, y_pred, output_dict=True)\n",
    "\n",
    "    result = {\n",
    "        category_name: {\n",
    "            'Model': model_type,\n",
    "            'Category': category_name,\n",
    "            'Data_Variant': data_variant,\n",
    "            'Accuracy': round(acc, 4),\n",
    "            'Precision': round(report['macro avg']['precision'], 4),\n",
    "            'Recall': round(report['macro avg']['recall'], 4),\n",
    "            'F1-Score': round(report['macro avg']['f1-score'], 4),\n",
    "        }\n",
    "    }\n",
    "    return result[category_name]\n",
    "\n",
    "def evaluate_all_models_combined(data_variants, model_types, version='v2'):\n",
    "    results = []\n",
    "\n",
    "    for model_type in model_types:\n",
    "        for variant in data_variants:\n",
    "            print(f\"{model_type.upper()} | {variant} | Version: {version}\")\n",
    "            base_path = f\"saved_models/{version}/{model_type}/\"\n",
    "            models_to_evaluate = []\n",
    "\n",
    "            if model_type == 'ml':\n",
    "                model_path = f\"{base_path}best_model_{variant}.pkl\"\n",
    "                try:\n",
    "                    model = joblib.load(model_path)\n",
    "                    models_to_evaluate.append((f\"ML - {variant}\", model))\n",
    "                except Exception as e:\n",
    "                    print(f\" ML-Modell nicht gefunden: {model_path}\")\n",
    "\n",
    "            elif model_type == 'ann':\n",
    "                model_path = f\"{base_path}model_{variant}.keras\"\n",
    "                try:\n",
    "                    model = load_model(model_path)\n",
    "                    models_to_evaluate.append((f\"ANN - {variant}\", model))\n",
    "                except Exception as e:\n",
    "                    print(f\" ANN-Modell nicht gefunden: {model_path}\")\n",
    "\n",
    "            elif model_type == 'cnn':\n",
    "                model_path_no_aug = f\"{base_path}model_without_augmentation_{variant}.keras\"\n",
    "                model_path_aug = f\"{base_path}model_with_augmentation_{variant}.keras\"\n",
    "\n",
    "                try:\n",
    "                    model = load_model(model_path_no_aug)\n",
    "                    models_to_evaluate.append((f\"CNN (ohne Aug.) - {variant}\", model))\n",
    "                except Exception as e:\n",
    "                    print(f\"CNN ohne Augmentierung nicht gefunden: {model_path_no_aug}\")\n",
    "\n",
    "                try:\n",
    "                    model = load_model(model_path_aug)\n",
    "                    models_to_evaluate.append((f\"CNN (mit Aug.) - {variant}\", model))\n",
    "                except Exception as e:\n",
    "                    print(f\"CNN mit Augmentierung nicht gefunden: {model_path_aug}\")\n",
    "\n",
    "            # Daten vorbereiten\n",
    "            if variant == 'gray':\n",
    "                X_train, X_test, _, y_test = prepare_combined_data(X_gray, y_full_gray)\n",
    "            else:\n",
    "                X_train, X_test, _, y_test = prepare_combined_data(X_wavelet, y_full_wavelet)\n",
    "\n",
    "            if model_type == 'ann':\n",
    "                scaler = StandardScaler()\n",
    "                X_train = scaler.fit_transform(X_train)\n",
    "                X_test = scaler.transform(X_test)\n",
    "\n",
    "            if model_type == 'cnn':\n",
    "                X_test = X_test.reshape(-1, global_image_size[0], global_image_size[1], 1) / 255.0\n",
    "\n",
    "            for model_label, model in models_to_evaluate:\n",
    "                result = evaluate_model(model, model_label, 'Combined', variant, X_test, y_test)\n",
    "                results.append(result)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_variants = ['gray', 'wavelet']\n",
    "model_types = ['ml', 'ann', 'cnn']\n",
    "\n",
    "results_v2_combined = evaluate_all_models_combined(data_variants, model_types)\n",
    "\n",
    "# Als DataFrame\n",
    "df_results_v2_combined = pd.DataFrame(results_v2_combined)\n",
    "\n",
    "# Optional gestylt\n",
    "df_results_v2_combined.style.set_caption(\"Tabelle 4.2: Vergleich der Modelle – Variante B (kombinierte Klassifikation)\") \\\n",
    "    .set_table_styles([{\n",
    "        'selector': 'caption',\n",
    "        'props': [('color', 'darkblue'), ('font-size', '15px')]\n",
    "    }])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Beispielhafte Vorhersagen\n",
    "\n",
    "\n",
    "***Ansatz A (Separate Klassifikation – Tomato, CNN)***\n",
    "- **Original-Kategorie:** Tomato Late_blight  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict_and_display(img_path, model_path, labels, global_image_size=(64, 64)):\n",
    "    # Bild vorbereiten\n",
    "    img_gray = Image.open(img_path).convert('L')\n",
    "    img_gray_resized = img_gray.resize(global_image_size)\n",
    "    img_array = np.array(img_gray_resized).reshape(1, global_image_size[0], global_image_size[1], 1) / 255.0\n",
    "\n",
    "    # Modell laden und Vorhersage machen\n",
    "    model = load_model(model_path)\n",
    "    prediction = model.predict(img_array)\n",
    "    predicted_class = np.argmax(prediction)\n",
    "\n",
    "    # Originalbild laden und umwandeln\n",
    "    original = cv2.imread(img_path)\n",
    "    original = cv2.cvtColor(original, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Anzeige nebeneinander (horizontal)\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "    axs[0].imshow(original)\n",
    "    axs[0].set_title(\"Originalbild\")\n",
    "    axs[0].axis('off')\n",
    "\n",
    "    axs[1].imshow(img_gray_resized, cmap='gray')\n",
    "    axs[1].set_title(f\"Vorverarbeitet\\n(Predicted: {labels[predicted_class]})\")\n",
    "    axs[1].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_tomato = ['Bacterial_spot', 'Late_blight', 'healthy']\n",
    "img_path = 'data/PlantVillage/Tomato___Late_blight/image (8).JPG'\n",
    "model_path = 'saved_models/v1/cnn/tomato/model_without_augmentation_Tomato_gray.keras'\n",
    "\n",
    "predict_and_display(img_path, model_path, labels_tomato)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Ansatz B (Kombinierte Klassifikation )***\n",
    "- **Original-Kategorie:** Corn  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\n",
    "    'Apple___Apple_scab',\n",
    "    'Apple___Black_rot',\n",
    "    'Apple___healthy',\n",
    "    'Corn___Common_rust',\n",
    "    'Corn___Northern_Leaf_Blight',\n",
    "    'Corn___healthy',\n",
    "    'Tomato___Bacterial_spot',\n",
    "    'Tomato___Late_blight',\n",
    "    'Tomato___healthy'\n",
    "]\n",
    "img_path = 'data/PlantVillage/Corn___Northern_Leaf_Blight/image (31).JPG'\n",
    "img_path=  'data/PlantVillage/Apple___Black_rot/image (11).JPG'\n",
    "model_path = 'saved_models/v2/cnn/model_without_augmentation_gray.keras'\n",
    "predict_and_display(img_path, model_path, labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
